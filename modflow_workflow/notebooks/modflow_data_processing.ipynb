{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flujo de Trabajo para Procesamiento de Datos MODFLOW\n",
        "\n",
        "Este notebook presenta un flujo de trabajo completo para el pre-procesamiento y post-procesamiento de datos de niveles observados y simulados obtenidos de modelos de aguas subterrÃ¡neas MODFLOW.\n",
        "\n",
        "## Estructura del Proyecto\n",
        "- `data/raw/`: Datos originales (archivos .hds, .cbb, .obs)\n",
        "- `data/processed/`: Datos procesados y limpiados\n",
        "- `data/output/`: Resultados y anÃ¡lisis finales\n",
        "- `scripts/`: Funciones auxiliares\n",
        "- `figures/`: GrÃ¡ficos y visualizaciones\n",
        "- `reports/`: Reportes generados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librerÃ­as necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar estilo de grÃ¡ficos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configurar directorios del proyecto\n",
        "PROJECT_ROOT = Path.cwd().parent\n",
        "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "DATA_OUTPUT = PROJECT_ROOT / \"data\" / \"output\"\n",
        "FIGURES_DIR = PROJECT_ROOT / \"figures\"\n",
        "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
        "\n",
        "# Crear directorios si no existen\n",
        "for directory in [DATA_RAW, DATA_PROCESSED, DATA_OUTPUT, FIGURES_DIR, REPORTS_DIR]:\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ… LibrerÃ­as importadas y directorios configurados correctamente\")\n",
        "print(f\"ğŸ“ Directorio del proyecto: {PROJECT_ROOT}\")\n",
        "print(f\"ğŸ“Š Datos originales: {DATA_RAW}\")\n",
        "print(f\"ğŸ”§ Datos procesados: {DATA_PROCESSED}\")\n",
        "print(f\"ğŸ“ˆ Resultados: {DATA_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Funciones de Pre-procesamiento\n",
        "\n",
        "### 1.1 Lectura de archivos MODFLOW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_modflow_head_file(file_path, nrow, ncol):\n",
        "    \"\"\"\n",
        "    Lee archivos de cabezas de MODFLOW (.hds)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Ruta al archivo .hds\n",
        "    nrow, ncol : int\n",
        "        NÃºmero de filas y columnas de la grilla\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    numpy.ndarray\n",
        "        Array 3D con las cabezas por estrato y tiempo\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            # Leer cabecera del archivo\n",
        "            header = np.fromfile(f, dtype=np.float32, count=1)\n",
        "            data = np.fromfile(f, dtype=np.float32)\n",
        "            \n",
        "        # Reshape data segÃºn dimensiones\n",
        "        nlay = len(data) // (nrow * ncol)\n",
        "        data_reshaped = data.reshape(nlay, nrow, ncol)\n",
        "        \n",
        "        print(f\"âœ… Archivo {file_path} leÃ­do correctamente\")\n",
        "        print(f\"   Dimensiones: {nlay} estratos, {nrow} filas, {ncol} columnas\")\n",
        "        \n",
        "        return data_reshaped\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error leyendo archivo {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def read_observation_file(file_path):\n",
        "    \"\"\"\n",
        "    Lee archivos de observaciones MODFLOW (.obs)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Ruta al archivo .obs\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame con observaciones\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Leer archivo de observaciones\n",
        "        obs_data = []\n",
        "        \n",
        "        with open(file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            \n",
        "        for line in lines:\n",
        "            if line.strip() and not line.startswith('#'):\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 3:\n",
        "                    obs_data.append({\n",
        "                        'well_id': parts[0],\n",
        "                        'time': float(parts[1]),\n",
        "                        'head': float(parts[2])\n",
        "                    })\n",
        "        \n",
        "        df = pd.DataFrame(obs_data)\n",
        "        df['datetime'] = pd.to_datetime(df['time'], unit='D', origin='1900-01-01')\n",
        "        \n",
        "        print(f\"âœ… Archivo de observaciones {file_path} leÃ­do correctamente\")\n",
        "        print(f\"   {len(df)} observaciones encontradas\")\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error leyendo archivo {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Ejemplo de uso\n",
        "print(\"ğŸ“‹ Funciones de lectura de archivos MODFLOW definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Limpieza y validaciÃ³n de datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_observation_data(df, min_head=-1000, max_head=1000):\n",
        "    \"\"\"\n",
        "    Limpia y valida datos de observaciones\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame con observaciones\n",
        "    min_head, max_head : float\n",
        "        Valores mÃ­nimos y mÃ¡ximos vÃ¡lidos para cabezas\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame limpio\n",
        "    \"\"\"\n",
        "    print(\"ğŸ§¹ Iniciando limpieza de datos de observaciones...\")\n",
        "    \n",
        "    # Copiar DataFrame\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Eliminar valores nulos\n",
        "    initial_count = len(df_clean)\n",
        "    df_clean = df_clean.dropna()\n",
        "    print(f\"   Eliminados {initial_count - len(df_clean)} registros con valores nulos\")\n",
        "    \n",
        "    # Filtrar valores fuera de rango\n",
        "    before_filter = len(df_clean)\n",
        "    df_clean = df_clean[(df_clean['head'] >= min_head) & (df_clean['head'] <= max_head)]\n",
        "    print(f\"   Eliminados {before_filter - len(df_clean)} registros fuera de rango [{min_head}, {max_head}]\")\n",
        "    \n",
        "    # Eliminar duplicados\n",
        "    before_dedup = len(df_clean)\n",
        "    df_clean = df_clean.drop_duplicates(subset=['well_id', 'time'])\n",
        "    print(f\"   Eliminados {before_dedup - len(df_clean)} registros duplicados\")\n",
        "    \n",
        "    # Ordenar por pozo y tiempo\n",
        "    df_clean = df_clean.sort_values(['well_id', 'time']).reset_index(drop=True)\n",
        "    \n",
        "    print(f\"âœ… Limpieza completada: {len(df_clean)} observaciones vÃ¡lidas\")\n",
        "    return df_clean\n",
        "\n",
        "def validate_head_data(head_array, nodata_value=-999.99):\n",
        "    \"\"\"\n",
        "    Valida datos de cabezas de MODFLOW\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    head_array : numpy.ndarray\n",
        "        Array 3D con cabezas\n",
        "    nodata_value : float\n",
        "        Valor que representa datos faltantes\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        EstadÃ­sticas de validaciÃ³n\n",
        "    \"\"\"\n",
        "    print(\"ğŸ” Validando datos de cabezas...\")\n",
        "    \n",
        "    # Calcular estadÃ­sticas\n",
        "    valid_data = head_array[head_array != nodata_value]\n",
        "    \n",
        "    stats = {\n",
        "        'total_cells': head_array.size,\n",
        "        'valid_cells': len(valid_data),\n",
        "        'nodata_cells': head_array.size - len(valid_data),\n",
        "        'min_head': np.min(valid_data) if len(valid_data) > 0 else None,\n",
        "        'max_head': np.max(valid_data) if len(valid_data) > 0 else None,\n",
        "        'mean_head': np.mean(valid_data) if len(valid_data) > 0 else None,\n",
        "        'std_head': np.std(valid_data) if len(valid_data) > 0 else None\n",
        "    }\n",
        "    \n",
        "    print(f\"   Celdas totales: {stats['total_cells']}\")\n",
        "    print(f\"   Celdas vÃ¡lidas: {stats['valid_cells']}\")\n",
        "    print(f\"   Celdas sin datos: {stats['nodata_cells']}\")\n",
        "    print(f\"   Rango de cabezas: {stats['min_head']:.2f} a {stats['max_head']:.2f}\")\n",
        "    \n",
        "    return stats\n",
        "\n",
        "print(\"ğŸ“‹ Funciones de limpieza y validaciÃ³n definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Funciones de Post-procesamiento\n",
        "\n",
        "### 2.1 AnÃ¡lisis estadÃ­stico\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_statistics(observed, simulated):\n",
        "    \"\"\"\n",
        "    Calcula estadÃ­sticas de comparaciÃ³n entre datos observados y simulados\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    observed : pandas.DataFrame\n",
        "        Datos observados\n",
        "    simulated : pandas.DataFrame\n",
        "        Datos simulados\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Diccionario con estadÃ­sticas\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“Š Calculando estadÃ­sticas de comparaciÃ³n...\")\n",
        "    \n",
        "    # Merge de datos por pozo y tiempo\n",
        "    merged = pd.merge(observed, simulated, on=['well_id', 'time'], \n",
        "                     suffixes=('_obs', '_sim'))\n",
        "    \n",
        "    # Calcular diferencias\n",
        "    merged['residual'] = merged['head_obs'] - merged['head_sim']\n",
        "    merged['abs_residual'] = np.abs(merged['residual'])\n",
        "    merged['squared_residual'] = merged['residual']**2\n",
        "    \n",
        "    # EstadÃ­sticas globales\n",
        "    stats = {\n",
        "        'n_observations': len(merged),\n",
        "        'mean_obs': merged['head_obs'].mean(),\n",
        "        'mean_sim': merged['head_sim'].mean(),\n",
        "        'std_obs': merged['head_obs'].std(),\n",
        "        'std_sim': merged['head_sim'].std(),\n",
        "        'mean_residual': merged['residual'].mean(),\n",
        "        'std_residual': merged['residual'].std(),\n",
        "        'mae': merged['abs_residual'].mean(),  # Mean Absolute Error\n",
        "        'rmse': np.sqrt(merged['squared_residual'].mean()),  # Root Mean Square Error\n",
        "        'r2': 1 - (merged['squared_residual'].sum() / \n",
        "                  ((merged['head_obs'] - merged['head_obs'].mean())**2).sum()),\n",
        "        'nse': 1 - (merged['squared_residual'].sum() / \n",
        "                   ((merged['head_obs'] - merged['head_obs'].mean())**2).sum())  # Nash-Sutcliffe Efficiency\n",
        "    }\n",
        "    \n",
        "    print(f\"   Observaciones: {stats['n_observations']}\")\n",
        "    print(f\"   MAE: {stats['mae']:.3f}\")\n",
        "    print(f\"   RMSE: {stats['rmse']:.3f}\")\n",
        "    print(f\"   RÂ²: {stats['r2']:.3f}\")\n",
        "    print(f\"   NSE: {stats['nse']:.3f}\")\n",
        "    \n",
        "    return stats, merged\n",
        "\n",
        "def analyze_by_well(merged_data):\n",
        "    \"\"\"\n",
        "    Analiza estadÃ­sticas por pozo individual\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    merged_data : pandas.DataFrame\n",
        "        Datos fusionados observados-simulados\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        EstadÃ­sticas por pozo\n",
        "    \"\"\"\n",
        "    print(\"ğŸ” Analizando estadÃ­sticas por pozo...\")\n",
        "    \n",
        "    well_stats = []\n",
        "    \n",
        "    for well_id in merged_data['well_id'].unique():\n",
        "        well_data = merged_data[merged_data['well_id'] == well_id]\n",
        "        \n",
        "        if len(well_data) > 1:  # Necesitamos al menos 2 puntos para estadÃ­sticas\n",
        "            stats = {\n",
        "                'well_id': well_id,\n",
        "                'n_obs': len(well_data),\n",
        "                'mae': well_data['abs_residual'].mean(),\n",
        "                'rmse': np.sqrt(well_data['squared_residual'].mean()),\n",
        "                'r2': 1 - (well_data['squared_residual'].sum() / \n",
        "                          ((well_data['head_obs'] - well_data['head_obs'].mean())**2).sum()),\n",
        "                'mean_residual': well_data['residual'].mean(),\n",
        "                'std_residual': well_data['residual'].std()\n",
        "            }\n",
        "            well_stats.append(stats)\n",
        "    \n",
        "    well_df = pd.DataFrame(well_stats)\n",
        "    print(f\"   Analizados {len(well_df)} pozos\")\n",
        "    \n",
        "    return well_df\n",
        "\n",
        "print(\"ğŸ“‹ Funciones de anÃ¡lisis estadÃ­stico definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Visualizaciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_time_series(merged_data, well_ids=None, save_path=None):\n",
        "    \"\"\"\n",
        "    Genera grÃ¡ficos de series temporales para pozos especÃ­ficos\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    merged_data : pandas.DataFrame\n",
        "        Datos fusionados observados-simulados\n",
        "    well_ids : list, optional\n",
        "        Lista de IDs de pozos a graficar\n",
        "    save_path : str, optional\n",
        "        Ruta para guardar el grÃ¡fico\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“ˆ Generando grÃ¡ficos de series temporales...\")\n",
        "    \n",
        "    if well_ids is None:\n",
        "        well_ids = merged_data['well_id'].unique()[:5]  # Primeros 5 pozos por defecto\n",
        "    \n",
        "    n_wells = len(well_ids)\n",
        "    fig, axes = plt.subplots(n_wells, 1, figsize=(12, 3*n_wells))\n",
        "    \n",
        "    if n_wells == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, well_id in enumerate(well_ids):\n",
        "        well_data = merged_data[merged_data['well_id'] == well_id].sort_values('time')\n",
        "        \n",
        "        axes[i].plot(well_data['time'], well_data['head_obs'], 'o-', \n",
        "                    label='Observado', color='blue', alpha=0.7)\n",
        "        axes[i].plot(well_data['time'], well_data['head_sim'], 's-', \n",
        "                    label='Simulado', color='red', alpha=0.7)\n",
        "        \n",
        "        axes[i].set_title(f'Pozo {well_id}')\n",
        "        axes[i].set_xlabel('Tiempo (dÃ­as)')\n",
        "        axes[i].set_ylabel('Cabeza (m)')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   GrÃ¡fico guardado en: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter_comparison(merged_data, save_path=None):\n",
        "    \"\"\"\n",
        "    Genera grÃ¡fico de dispersiÃ³n observado vs simulado\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    merged_data : pandas.DataFrame\n",
        "        Datos fusionados observados-simulados\n",
        "    save_path : str, optional\n",
        "        Ruta para guardar el grÃ¡fico\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“Š Generando grÃ¡fico de dispersiÃ³n...\")\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "    \n",
        "    # Scatter plot\n",
        "    ax.scatter(merged_data['head_obs'], merged_data['head_sim'], \n",
        "              alpha=0.6, s=50, color='blue')\n",
        "    \n",
        "    # LÃ­nea 1:1\n",
        "    min_val = min(merged_data['head_obs'].min(), merged_data['head_sim'].min())\n",
        "    max_val = max(merged_data['head_obs'].max(), merged_data['head_sim'].max())\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', \n",
        "           label='LÃ­nea 1:1', linewidth=2)\n",
        "    \n",
        "    # LÃ­nea de regresiÃ³n\n",
        "    z = np.polyfit(merged_data['head_obs'], merged_data['head_sim'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax.plot(merged_data['head_obs'], p(merged_data['head_obs']), \n",
        "           'g-', label=f'RegresiÃ³n (RÂ²={z[0]:.3f})', linewidth=2)\n",
        "    \n",
        "    ax.set_xlabel('Cabeza Observada (m)')\n",
        "    ax.set_ylabel('Cabeza Simulada (m)')\n",
        "    ax.set_title('ComparaciÃ³n Observado vs Simulado')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   GrÃ¡fico guardado en: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def plot_residuals(merged_data, save_path=None):\n",
        "    \"\"\"\n",
        "    Genera grÃ¡ficos de anÃ¡lisis de residuos\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    merged_data : pandas.DataFrame\n",
        "        Datos fusionados observados-simulados\n",
        "    save_path : str, optional\n",
        "        Ruta para guardar el grÃ¡fico\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“‰ Generando anÃ¡lisis de residuos...\")\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # 1. Residuos vs tiempo\n",
        "    axes[0,0].scatter(merged_data['time'], merged_data['residual'], alpha=0.6)\n",
        "    axes[0,0].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[0,0].set_xlabel('Tiempo (dÃ­as)')\n",
        "    axes[0,0].set_ylabel('Residuo (m)')\n",
        "    axes[0,0].set_title('Residuos vs Tiempo')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Residuos vs valores observados\n",
        "    axes[0,1].scatter(merged_data['head_obs'], merged_data['residual'], alpha=0.6)\n",
        "    axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[0,1].set_xlabel('Cabeza Observada (m)')\n",
        "    axes[0,1].set_ylabel('Residuo (m)')\n",
        "    axes[0,1].set_title('Residuos vs Valores Observados')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Histograma de residuos\n",
        "    axes[1,0].hist(merged_data['residual'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[1,0].axvline(x=0, color='r', linestyle='--')\n",
        "    axes[1,0].set_xlabel('Residuo (m)')\n",
        "    axes[1,0].set_ylabel('Frecuencia')\n",
        "    axes[1,0].set_title('DistribuciÃ³n de Residuos')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Q-Q plot\n",
        "    from scipy import stats\n",
        "    stats.probplot(merged_data['residual'], dist=\"norm\", plot=axes[1,1])\n",
        "    axes[1,1].set_title('Q-Q Plot de Residuos')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   GrÃ¡fico guardado en: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "print(\"ğŸ“‹ Funciones de visualizaciÃ³n definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ejemplo de Uso con Datos SintÃ©ticos\n",
        "\n",
        "### 3.1 GeneraciÃ³n de datos de ejemplo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar datos sintÃ©ticos para demostraciÃ³n\n",
        "print(\"ğŸ”§ Generando datos sintÃ©ticos para demostraciÃ³n...\")\n",
        "\n",
        "# ParÃ¡metros del ejemplo\n",
        "n_wells = 5\n",
        "n_times = 100\n",
        "time_start = 0\n",
        "time_end = 365\n",
        "\n",
        "# Generar datos observados (con ruido)\n",
        "np.random.seed(42)  # Para reproducibilidad\n",
        "\n",
        "observed_data = []\n",
        "for well_id in range(1, n_wells + 1):\n",
        "    # Tendencia base diferente para cada pozo\n",
        "    base_head = 100 + well_id * 5  # Diferentes niveles base\n",
        "    seasonal = 2 * np.sin(2 * np.pi * np.linspace(0, 1, n_times))  # VariaciÃ³n estacional\n",
        "    trend = 0.1 * np.linspace(0, 1, n_times)  # Tendencia temporal\n",
        "    noise = np.random.normal(0, 0.5, n_times)  # Ruido aleatorio\n",
        "    \n",
        "    times = np.linspace(time_start, time_end, n_times)\n",
        "    heads = base_head + seasonal + trend + noise\n",
        "    \n",
        "    for t, h in zip(times, heads):\n",
        "        observed_data.append({\n",
        "            'well_id': f'PZ{well_id:02d}',\n",
        "            'time': t,\n",
        "            'head': h\n",
        "        })\n",
        "\n",
        "observed_df = pd.DataFrame(observed_data)\n",
        "\n",
        "# Generar datos simulados (con algunos errores sistemÃ¡ticos)\n",
        "simulated_data = []\n",
        "for well_id in range(1, n_wells + 1):\n",
        "    well_obs = observed_df[observed_df['well_id'] == f'PZ{well_id:02d}']\n",
        "    \n",
        "    for _, row in well_obs.iterrows():\n",
        "        # Simular error sistemÃ¡tico y aleatorio\n",
        "        systematic_error = 0.5 if well_id % 2 == 0 else -0.3  # Error sistemÃ¡tico por pozo\n",
        "        random_error = np.random.normal(0, 0.3)\n",
        "        simulated_head = row['head'] + systematic_error + random_error\n",
        "        \n",
        "        simulated_data.append({\n",
        "            'well_id': row['well_id'],\n",
        "            'time': row['time'],\n",
        "            'head': simulated_head\n",
        "        })\n",
        "\n",
        "simulated_df = pd.DataFrame(simulated_data)\n",
        "\n",
        "print(f\"âœ… Datos sintÃ©ticos generados:\")\n",
        "print(f\"   Pozos: {n_wells}\")\n",
        "print(f\"   Observaciones por pozo: {n_times}\")\n",
        "print(f\"   Total observaciones: {len(observed_df)}\")\n",
        "print(f\"   Rango de cabezas observadas: {observed_df['head'].min():.2f} - {observed_df['head'].max():.2f} m\")\n",
        "print(f\"   Rango de cabezas simuladas: {simulated_df['head'].min():.2f} - {simulated_df['head'].max():.2f} m\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 AplicaciÃ³n del flujo de trabajo completo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PASO 1: Limpiar datos observados\n",
        "print(\"=\" * 60)\n",
        "print(\"PASO 1: PRE-PROCESAMIENTO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "observed_clean = clean_observation_data(observed_df)\n",
        "simulated_clean = clean_observation_data(simulated_df)\n",
        "\n",
        "# PASO 2: Calcular estadÃ­sticas\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PASO 2: ANÃLISIS ESTADÃSTICO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "stats, merged_data = calculate_statistics(observed_clean, simulated_clean)\n",
        "well_stats = analyze_by_well(merged_data)\n",
        "\n",
        "# Mostrar estadÃ­sticas por pozo\n",
        "print(\"\\nğŸ“Š EstadÃ­sticas por pozo:\")\n",
        "print(well_stats.round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PASO 3: Generar visualizaciones\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PASO 3: VISUALIZACIONES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# GrÃ¡fico de series temporales\n",
        "plot_time_series(merged_data, well_ids=['PZ01', 'PZ02', 'PZ03'], \n",
        "                save_path=FIGURES_DIR / \"time_series.png\")\n",
        "\n",
        "# GrÃ¡fico de dispersiÃ³n\n",
        "plot_scatter_comparison(merged_data, \n",
        "                       save_path=FIGURES_DIR / \"scatter_comparison.png\")\n",
        "\n",
        "# AnÃ¡lisis de residuos\n",
        "plot_residuals(merged_data, \n",
        "              save_path=FIGURES_DIR / \"residual_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PASO 4: Guardar resultados\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PASO 4: GUARDAR RESULTADOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Guardar datos procesados\n",
        "observed_clean.to_csv(DATA_PROCESSED / \"observed_clean.csv\", index=False)\n",
        "simulated_clean.to_csv(DATA_PROCESSED / \"simulated_clean.csv\", index=False)\n",
        "merged_data.to_csv(DATA_OUTPUT / \"merged_data.csv\", index=False)\n",
        "well_stats.to_csv(DATA_OUTPUT / \"well_statistics.csv\", index=False)\n",
        "\n",
        "# Guardar estadÃ­sticas globales\n",
        "stats_df = pd.DataFrame([stats])\n",
        "stats_df.to_csv(DATA_OUTPUT / \"global_statistics.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Archivos guardados:\")\n",
        "print(f\"   Datos observados limpios: {DATA_PROCESSED / 'observed_clean.csv'}\")\n",
        "print(f\"   Datos simulados limpios: {DATA_PROCESSED / 'simulated_clean.csv'}\")\n",
        "print(f\"   Datos fusionados: {DATA_OUTPUT / 'merged_data.csv'}\")\n",
        "print(f\"   EstadÃ­sticas por pozo: {DATA_OUTPUT / 'well_statistics.csv'}\")\n",
        "print(f\"   EstadÃ­sticas globales: {DATA_OUTPUT / 'global_statistics.csv'}\")\n",
        "print(f\"   GrÃ¡ficos: {FIGURES_DIR}\")\n",
        "\n",
        "# Resumen final\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RESUMEN FINAL\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ğŸ“Š Total de observaciones analizadas: {stats['n_observations']}\")\n",
        "print(f\"ğŸ“ˆ Error absoluto medio (MAE): {stats['mae']:.3f} m\")\n",
        "print(f\"ğŸ“‰ Error cuadrÃ¡tico medio (RMSE): {stats['rmse']:.3f} m\")\n",
        "print(f\"ğŸ¯ Coeficiente de determinaciÃ³n (RÂ²): {stats['r2']:.3f}\")\n",
        "print(f\"âš¡ Eficiencia Nash-Sutcliffe (NSE): {stats['nse']:.3f}\")\n",
        "\n",
        "# InterpretaciÃ³n de resultados\n",
        "print(\"\\nğŸ“‹ InterpretaciÃ³n de resultados:\")\n",
        "if stats['nse'] > 0.7:\n",
        "    print(\"   âœ… Excelente ajuste del modelo (NSE > 0.7)\")\n",
        "elif stats['nse'] > 0.5:\n",
        "    print(\"   âš ï¸  Ajuste aceptable del modelo (NSE > 0.5)\")\n",
        "else:\n",
        "    print(\"   âŒ Ajuste deficiente del modelo (NSE < 0.5)\")\n",
        "\n",
        "if stats['r2'] > 0.8:\n",
        "    print(\"   âœ… Alta correlaciÃ³n entre observado y simulado (RÂ² > 0.8)\")\n",
        "elif stats['r2'] > 0.6:\n",
        "    print(\"   âš ï¸  CorrelaciÃ³n moderada (RÂ² > 0.6)\")\n",
        "else:\n",
        "    print(\"   âŒ Baja correlaciÃ³n (RÂ² < 0.6)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Funciones Auxiliares para Archivos MODFLOW\n",
        "\n",
        "### 4.1 Procesamiento de archivos .hds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_head_at_wells(head_array, well_locations, nrow, ncol):\n",
        "    \"\"\"\n",
        "    Extrae valores de cabeza en ubicaciones especÃ­ficas de pozos\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    head_array : numpy.ndarray\n",
        "        Array 3D con cabezas\n",
        "    well_locations : list\n",
        "        Lista de tuplas (row, col) para cada pozo\n",
        "    nrow, ncol : int\n",
        "        Dimensiones de la grilla\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame con cabezas por pozo y estrato\n",
        "    \"\"\"\n",
        "    print(\"ğŸ” Extrayendo cabezas en ubicaciones de pozos...\")\n",
        "    \n",
        "    well_data = []\n",
        "    \n",
        "    for i, (row, col) in enumerate(well_locations):\n",
        "        if 0 <= row < nrow and 0 <= col < ncol:\n",
        "            for layer in range(head_array.shape[0]):\n",
        "                head_value = head_array[layer, row, col]\n",
        "                well_data.append({\n",
        "                    'well_id': f'PZ{i+1:02d}',\n",
        "                    'layer': layer + 1,\n",
        "                    'row': row + 1,  # Convertir a Ã­ndices 1-based\n",
        "                    'col': col + 1,\n",
        "                    'head': head_value\n",
        "                })\n",
        "        else:\n",
        "            print(f\"   âš ï¸  UbicaciÃ³n de pozo {i+1} fuera de rango: ({row}, {col})\")\n",
        "    \n",
        "    df = pd.DataFrame(well_data)\n",
        "    print(f\"   âœ… ExtraÃ­das {len(df)} cabezas de {len(well_locations)} pozos\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def create_head_contour_plot(head_array, layer=0, save_path=None):\n",
        "    \"\"\"\n",
        "    Crea mapa de contornos de cabezas\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    head_array : numpy.ndarray\n",
        "        Array 3D con cabezas\n",
        "    layer : int\n",
        "        Estrato a visualizar\n",
        "    save_path : str, optional\n",
        "        Ruta para guardar el grÃ¡fico\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ—ºï¸  Generando mapa de contornos para estrato {layer+1}...\")\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "    \n",
        "    # Crear contornos\n",
        "    contour = ax.contour(head_array[layer], levels=20, colors='black', alpha=0.6)\n",
        "    contour_filled = ax.contourf(head_array[layer], levels=20, cmap='viridis', alpha=0.8)\n",
        "    \n",
        "    # AÃ±adir barra de color\n",
        "    cbar = plt.colorbar(contour_filled, ax=ax)\n",
        "    cbar.set_label('Cabeza (m)', rotation=270, labelpad=20)\n",
        "    \n",
        "    ax.set_title(f'Mapa de Contornos de Cabezas - Estrato {layer+1}')\n",
        "    ax.set_xlabel('Columna')\n",
        "    ax.set_ylabel('Fila')\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   Mapa guardado en: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "print(\"ğŸ“‹ Funciones auxiliares para archivos MODFLOW definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Flujo de Trabajo Completo\n",
        "\n",
        "### 5.1 FunciÃ³n principal de procesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_modflow_data(observed_file=None, simulated_file=None, head_file=None, \n",
        "                        well_locations=None, nrow=None, ncol=None, \n",
        "                        output_dir=None, project_name=\"modflow_analysis\"):\n",
        "    \"\"\"\n",
        "    FunciÃ³n principal para procesar datos de MODFLOW\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    observed_file : str, optional\n",
        "        Ruta al archivo de observaciones\n",
        "    simulated_file : str, optional\n",
        "        Ruta al archivo de datos simulados\n",
        "    head_file : str, optional\n",
        "        Ruta al archivo .hds\n",
        "    well_locations : list, optional\n",
        "        Ubicaciones de pozos [(row, col), ...]\n",
        "    nrow, ncol : int, optional\n",
        "        Dimensiones de la grilla\n",
        "    output_dir : str, optional\n",
        "        Directorio de salida\n",
        "    project_name : str\n",
        "        Nombre del proyecto\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Diccionario con resultados del anÃ¡lisis\n",
        "    \"\"\"\n",
        "    print(\"ğŸš€ Iniciando procesamiento completo de datos MODFLOW\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Configurar directorios\n",
        "    if output_dir is None:\n",
        "        output_dir = Path.cwd() / \"output\" / project_name\n",
        "    \n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    results = {\n",
        "        'project_name': project_name,\n",
        "        'output_dir': str(output_dir),\n",
        "        'processed_files': [],\n",
        "        'statistics': {},\n",
        "        'figures': []\n",
        "    }\n",
        "    \n",
        "    # PASO 1: Procesar datos observados\n",
        "    if observed_file and Path(observed_file).exists():\n",
        "        print(\"ğŸ“Š Procesando datos observados...\")\n",
        "        observed_df = read_observation_file(observed_file)\n",
        "        if observed_df is not None:\n",
        "            observed_clean = clean_observation_data(observed_df)\n",
        "            observed_clean.to_csv(output_dir / \"observed_clean.csv\", index=False)\n",
        "            results['processed_files'].append(\"observed_clean.csv\")\n",
        "            print(\"   âœ… Datos observados procesados\")\n",
        "    \n",
        "    # PASO 2: Procesar datos simulados\n",
        "    if simulated_file and Path(simulated_file).exists():\n",
        "        print(\"ğŸ”§ Procesando datos simulados...\")\n",
        "        simulated_df = read_observation_file(simulated_file)\n",
        "        if simulated_df is not None:\n",
        "            simulated_clean = clean_observation_data(simulated_df)\n",
        "            simulated_clean.to_csv(output_dir / \"simulated_clean.csv\", index=False)\n",
        "            results['processed_files'].append(\"simulated_clean.csv\")\n",
        "            print(\"   âœ… Datos simulados procesados\")\n",
        "    \n",
        "    # PASO 3: Procesar archivos de cabezas\n",
        "    if head_file and Path(head_file).exists() and nrow and ncol:\n",
        "        print(\"ğŸ—‚ï¸  Procesando archivo de cabezas...\")\n",
        "        head_array = read_modflow_head_file(head_file, nrow, ncol)\n",
        "        if head_array is not None:\n",
        "            # Validar datos\n",
        "            head_stats = validate_head_data(head_array)\n",
        "            results['head_statistics'] = head_stats\n",
        "            \n",
        "            # Extraer cabezas en pozos si se proporcionan ubicaciones\n",
        "            if well_locations:\n",
        "                well_heads = extract_head_at_wells(head_array, well_locations, nrow, ncol)\n",
        "                well_heads.to_csv(output_dir / \"well_heads.csv\", index=False)\n",
        "                results['processed_files'].append(\"well_heads.csv\")\n",
        "            \n",
        "            # Crear mapa de contornos\n",
        "            create_head_contour_plot(head_array, layer=0, \n",
        "                                   save_path=output_dir / \"head_contour.png\")\n",
        "            results['figures'].append(\"head_contour.png\")\n",
        "            print(\"   âœ… Archivo de cabezas procesado\")\n",
        "    \n",
        "    # PASO 4: AnÃ¡lisis estadÃ­stico (si tenemos datos observados y simulados)\n",
        "    if 'observed_clean' in locals() and 'simulated_clean' in locals():\n",
        "        print(\"ğŸ“ˆ Realizando anÃ¡lisis estadÃ­stico...\")\n",
        "        stats, merged_data = calculate_statistics(observed_clean, simulated_clean)\n",
        "        well_stats = analyze_by_well(merged_data)\n",
        "        \n",
        "        # Guardar resultados\n",
        "        merged_data.to_csv(output_dir / \"merged_data.csv\", index=False)\n",
        "        well_stats.to_csv(output_dir / \"well_statistics.csv\", index=False)\n",
        "        stats_df = pd.DataFrame([stats])\n",
        "        stats_df.to_csv(output_dir / \"global_statistics.csv\", index=False)\n",
        "        \n",
        "        results['statistics'] = stats\n",
        "        results['processed_files'].extend([\"merged_data.csv\", \"well_statistics.csv\", \"global_statistics.csv\"])\n",
        "        \n",
        "        # Generar visualizaciones\n",
        "        print(\"ğŸ“Š Generando visualizaciones...\")\n",
        "        plot_time_series(merged_data, save_path=output_dir / \"time_series.png\")\n",
        "        plot_scatter_comparison(merged_data, save_path=output_dir / \"scatter_comparison.png\")\n",
        "        plot_residuals(merged_data, save_path=output_dir / \"residual_analysis.png\")\n",
        "        \n",
        "        results['figures'].extend([\"time_series.png\", \"scatter_comparison.png\", \"residual_analysis.png\"])\n",
        "        print(\"   âœ… AnÃ¡lisis estadÃ­stico completado\")\n",
        "    \n",
        "    # PASO 5: Generar reporte\n",
        "    print(\"ğŸ“ Generando reporte...\")\n",
        "    generate_report(results, output_dir / \"analysis_report.txt\")\n",
        "    results['processed_files'].append(\"analysis_report.txt\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âœ… PROCESAMIENTO COMPLETADO\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ğŸ“ Directorio de salida: {output_dir}\")\n",
        "    print(f\"ğŸ“Š Archivos procesados: {len(results['processed_files'])}\")\n",
        "    print(f\"ğŸ“ˆ GrÃ¡ficos generados: {len(results['figures'])}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "def generate_report(results, report_path):\n",
        "    \"\"\"\n",
        "    Genera un reporte de texto con los resultados del anÃ¡lisis\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    results : dict\n",
        "        Resultados del anÃ¡lisis\n",
        "    report_path : str\n",
        "        Ruta para guardar el reporte\n",
        "    \"\"\"\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"REPORTE DE ANÃLISIS MODFLOW\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"Proyecto: {results['project_name']}\\n\")\n",
        "        f.write(f\"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "        \n",
        "        f.write(\"ARCHIVOS PROCESADOS:\\n\")\n",
        "        f.write(\"-\" * 30 + \"\\n\")\n",
        "        for file in results['processed_files']:\n",
        "            f.write(f\"â€¢ {file}\\n\")\n",
        "        \n",
        "        f.write(\"\\nGRÃFICOS GENERADOS:\\n\")\n",
        "        f.write(\"-\" * 30 + \"\\n\")\n",
        "        for fig in results['figures']:\n",
        "            f.write(f\"â€¢ {fig}\\n\")\n",
        "        \n",
        "        if 'statistics' in results and results['statistics']:\n",
        "            f.write(\"\\nESTADÃSTICAS GLOBALES:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            stats = results['statistics']\n",
        "            f.write(f\"Observaciones: {stats.get('n_observations', 'N/A')}\\n\")\n",
        "            f.write(f\"MAE: {stats.get('mae', 'N/A'):.3f} m\\n\")\n",
        "            f.write(f\"RMSE: {stats.get('rmse', 'N/A'):.3f} m\\n\")\n",
        "            f.write(f\"RÂ²: {stats.get('r2', 'N/A'):.3f}\\n\")\n",
        "            f.write(f\"NSE: {stats.get('nse', 'N/A'):.3f}\\n\")\n",
        "    \n",
        "    print(f\"   ğŸ“„ Reporte guardado en: {report_path}\")\n",
        "\n",
        "print(\"ğŸ“‹ FunciÃ³n principal de procesamiento definida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Instrucciones de Uso\n",
        "\n",
        "### 6.1 Para usar con datos reales:\n",
        "\n",
        "```python\n",
        "# Ejemplo de uso con archivos reales\n",
        "results = process_modflow_data(\n",
        "    observed_file=\"data/raw/observations.obs\",\n",
        "    simulated_file=\"data/raw/simulated.obs\", \n",
        "    head_file=\"data/raw/model.hds\",\n",
        "    well_locations=[(10, 15), (20, 25), (30, 35)],  # (fila, columna)\n",
        "    nrow=50, ncol=60,\n",
        "    output_dir=\"results/analysis_2024\",\n",
        "    project_name=\"mi_proyecto_modflow\"\n",
        ")\n",
        "```\n",
        "\n",
        "### 6.2 Estructura de archivos de entrada:\n",
        "\n",
        "**Archivo de observaciones (.obs):**\n",
        "```\n",
        "# Comentarios opcionales\n",
        "PZ01 0.0 105.2\n",
        "PZ01 30.0 104.8\n",
        "PZ02 0.0 98.5\n",
        "PZ02 30.0 98.1\n",
        "```\n",
        "\n",
        "**Archivo de cabezas (.hds):**\n",
        "- Archivo binario generado por MODFLOW\n",
        "- Requiere especificar dimensiones de la grilla (nrow, ncol)\n",
        "\n",
        "### 6.3 InterpretaciÃ³n de resultados:\n",
        "\n",
        "- **MAE (Mean Absolute Error)**: Error absoluto medio en metros\n",
        "- **RMSE (Root Mean Square Error)**: Error cuadrÃ¡tico medio en metros  \n",
        "- **RÂ²**: Coeficiente de determinaciÃ³n (0-1, mayor es mejor)\n",
        "- **NSE (Nash-Sutcliffe Efficiency)**: Eficiencia del modelo (-âˆ a 1, mayor es mejor)\n",
        "\n",
        "**Criterios de evaluaciÃ³n:**\n",
        "- NSE > 0.7: Excelente ajuste\n",
        "- NSE > 0.5: Ajuste aceptable  \n",
        "- NSE < 0.5: Ajuste deficiente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Archivos de ConfiguraciÃ³n\n",
        "\n",
        "### 7.1 requirements.txt\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
