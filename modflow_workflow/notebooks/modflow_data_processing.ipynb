{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flujo de Trabajo para Procesamiento de Datos MODFLOW\n",
        "\n",
        "Este notebook presenta un flujo de trabajo completo para el pre-procesamiento y post-procesamiento de datos de niveles observados y simulados obtenidos de modelos de aguas subterr√°neas MODFLOW.\n",
        "\n",
        "## Estructura del Proyecto\n",
        "- `data/raw/`: Datos originales (archivos .hds, .cbb, .obs)\n",
        "- `data/processed/`: Datos procesados y limpiados\n",
        "- `data/output/`: Resultados y an√°lisis finales\n",
        "- `scripts/`: Funciones auxiliares\n",
        "- `figures/`: Gr√°ficos y visualizaciones\n",
        "- `reports/`: Reportes generados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librer√≠as necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar estilo de gr√°ficos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configurar directorios del proyecto\n",
        "PROJECT_ROOT = Path.cwd().parent\n",
        "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "DATA_OUTPUT = PROJECT_ROOT / \"data\" / \"output\"\n",
        "FIGURES_DIR = PROJECT_ROOT / \"figures\"\n",
        "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
        "\n",
        "# Crear directorios si no existen\n",
        "for directory in [DATA_RAW, DATA_PROCESSED, DATA_OUTPUT, FIGURES_DIR, REPORTS_DIR]:\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas y directorios configurados correctamente\")\n",
        "print(f\"üìÅ Directorio del proyecto: {PROJECT_ROOT}\")\n",
        "print(f\"üìä Datos originales: {DATA_RAW}\")\n",
        "print(f\"üîß Datos procesados: {DATA_PROCESSED}\")\n",
        "print(f\"üìà Resultados: {DATA_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Funciones de Pre-procesamiento\n",
        "\n",
        "### 1.1 Lectura de archivos MODFLOW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_modflow_head_file(file_path, nrow, ncol):\n",
        "    \"\"\"\n",
        "    Lee archivos de cabezas de MODFLOW (.hds)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Ruta al archivo .hds\n",
        "    nrow, ncol : int\n",
        "        N√∫mero de filas y columnas de la grilla\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    numpy.ndarray\n",
        "        Array 3D con las cabezas por estrato y tiempo\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            # Leer cabecera del archivo\n",
        "            header = np.fromfile(f, dtype=np.float32, count=1)\n",
        "            data = np.fromfile(f, dtype=np.float32)\n",
        "            \n",
        "        # Reshape data seg√∫n dimensiones\n",
        "        nlay = len(data) // (nrow * ncol)\n",
        "        data_reshaped = data.reshape(nlay, nrow, ncol)\n",
        "        \n",
        "        print(f\"‚úÖ Archivo {file_path} le√≠do correctamente\")\n",
        "        print(f\"   Dimensiones: {nlay} estratos, {nrow} filas, {ncol} columnas\")\n",
        "        \n",
        "        return data_reshaped\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error leyendo archivo {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def read_observation_file(file_path):\n",
        "    \"\"\"\n",
        "    Lee archivos de observaciones MODFLOW (.obs)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Ruta al archivo .obs\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame con observaciones\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Leer archivo de observaciones\n",
        "        obs_data = []\n",
        "        \n",
        "        with open(file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            \n",
        "        for line in lines:\n",
        "            if line.strip() and not line.startswith('#'):\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 3:\n",
        "                    obs_data.append({\n",
        "                        'well_id': parts[0],\n",
        "                        'time': float(parts[1]),\n",
        "                        'head': float(parts[2])\n",
        "                    })\n",
        "        \n",
        "        df = pd.DataFrame(obs_data)\n",
        "        df['datetime'] = pd.to_datetime(df['time'], unit='D', origin='1900-01-01')\n",
        "        \n",
        "        print(f\"‚úÖ Archivo de observaciones {file_path} le√≠do correctamente\")\n",
        "        print(f\"   {len(df)} observaciones encontradas\")\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error leyendo archivo {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Ejemplo de uso\n",
        "print(\"üìã Funciones de lectura de archivos MODFLOW definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Limpieza y validaci√≥n de datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_observation_data(df, min_head=-1000, max_head=1000):\n",
        "    \"\"\"\n",
        "    Limpia y valida datos de observaciones\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame con observaciones\n",
        "    min_head, max_head : float\n",
        "        Valores m√≠nimos y m√°ximos v√°lidos para cabezas\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame limpio\n",
        "    \"\"\"\n",
        "    print(\"üßπ Iniciando limpieza de datos de observaciones...\")\n",
        "    \n",
        "    # Copiar DataFrame\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Eliminar valores nulos\n",
        "    initial_count = len(df_clean)\n",
        "    df_clean = df_clean.dropna()\n",
        "    print(f\"   Eliminados {initial_count - len(df_clean)} registros con valores nulos\")\n",
        "    \n",
        "    # Filtrar valores fuera de rango\n",
        "    before_filter = len(df_clean)\n",
        "    df_clean = df_clean[(df_clean['head'] >= min_head) & (df_clean['head'] <= max_head)]\n",
        "    print(f\"   Eliminados {before_filter - len(df_clean)} registros fuera de rango [{min_head}, {max_head}]\")\n",
        "    \n",
        "    # Eliminar duplicados\n",
        "    before_dedup = len(df_clean)\n",
        "    df_clean = df_clean.drop_duplicates(subset=['well_id', 'time'])\n",
        "    print(f\"   Eliminados {before_dedup - len(df_clean)} registros duplicados\")\n",
        "    \n",
        "    # Ordenar por pozo y tiempo\n",
        "    df_clean = df_clean.sort_values(['well_id', 'time']).reset_index(drop=True)\n",
        "    \n",
        "    print(f\"‚úÖ Limpieza completada: {len(df_clean)} observaciones v√°lidas\")\n",
        "    return df_clean\n",
        "\n",
        "def validate_head_data(head_array, nodata_value=-999.99):\n",
        "    \"\"\"\n",
        "    Valida datos de cabezas de MODFLOW\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    head_array : numpy.ndarray\n",
        "        Array 3D con cabezas\n",
        "    nodata_value : float\n",
        "        Valor que representa datos faltantes\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Estad√≠sticas de validaci√≥n\n",
        "    \"\"\"\n",
        "    print(\"üîç Validando datos de cabezas...\")\n",
        "    \n",
        "    # Calcular estad√≠sticas\n",
        "    valid_data = head_array[head_array != nodata_value]\n",
        "    \n",
        "    stats = {\n",
        "        'total_cells': head_array.size,\n",
        "        'valid_cells': len(valid_data),\n",
        "        'nodata_cells': head_array.size - len(valid_data),\n",
        "        'min_head': np.min(valid_data) if len(valid_data) > 0 else None,\n",
        "        'max_head': np.max(valid_data) if len(valid_data) > 0 else None,\n",
        "        'mean_head': np.mean(valid_data) if len(valid_data) > 0 else None,\n",
        "        'std_head': np.std(valid_data) if len(valid_data) > 0 else None\n",
        "    }\n",
        "    \n",
        "    print(f\"   Celdas totales: {stats['total_cells']}\")\n",
        "    print(f\"   Celdas v√°lidas: {stats['valid_cells']}\")\n",
        "    print(f\"   Celdas sin datos: {stats['nodata_cells']}\")\n",
        "    print(f\"   Rango de cabezas: {stats['min_head']:.2f} a {stats['max_head']:.2f}\")\n",
        "    \n",
        "    return stats\n",
        "\n",
        "print(\"üìã Funciones de limpieza y validaci√≥n definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Funciones de Post-procesamiento\n",
        "\n",
        "### 2.1 An√°lisis estad√≠stico\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_statistics(observed, simulated):\n",
        "    \"\"\"\n",
        "    Calcula estad√≠sticas de comparaci√≥n entre datos observados y simulados\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    observed : pandas.DataFrame\n",
        "        Datos observados\n",
        "    simulated : pandas.DataFrame\n",
        "        Datos simulados\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Diccionario con estad√≠sticas\n",
        "    \"\"\"\n",
        "    print(\"üìä Calculando estad√≠sticas de comparaci√≥n...\")\n",
        "    \n",
        "    # Merge de datos por pozo y tiempo\n",
        "    merged = pd.merge(observed, simulated, on=['well_id', 'time'], \n",
        "                     suffixes=('_obs', '_sim'))\n",
        "    \n",
        "    # Calcular diferencias\n",
        "    merged['residual'] = merged['head_obs'] - merged['head_sim']\n",
        "    merged['abs_residual'] = np.abs(merged['residual'])\n",
        "    merged['squared_residual'] = merged['residual']**2\n",
        "    \n",
        "    # Estad√≠sticas globales\n",
        "    stats = {\n",
        "        'n_observations': len(merged),\n",
        "        'mean_obs': merged['head_obs'].mean(),\n",
        "        'mean_sim': merged['head_sim'].mean(),\n",
        "        'std_obs': merged['head_obs'].std(),\n",
        "        'std_sim': merged['head_sim'].std(),\n",
        "        'mean_residual': merged['residual'].mean(),\n",
        "        'std_residual': merged['residual'].std(),\n",
        "        'mae': merged['abs_residual'].mean(),  # Mean Absolute Error\n",
        "        'rmse': np.sqrt(merged['squared_residual'].mean()),  # Root Mean Square Error\n",
        "        'r2': 1 - (merged['squared_residual'].sum() / \n",
        "                  ((merged['head_obs'] - merged['head_obs'].mean())**2).sum()),\n",
        "        'nse': 1 - (merged['squared_residual'].sum() / \n",
        "                   ((merged['head_obs'] - merged['head_obs'].mean())**2).sum())  # Nash-Sutcliffe Efficiency\n",
        "    }\n",
        "    \n",
        "    print(f\"   Observaciones: {stats['n_observations']}\")\n",
        "    print(f\"   MAE: {stats['mae']:.3f}\")\n",
        "    print(f\"   RMSE: {stats['rmse']:.3f}\")\n",
        "    print(f\"   R¬≤: {stats['r2']:.3f}\")\n",
        "    print(f\"   NSE: {stats['nse']:.3f}\")\n",
        "    \n",
        "    return stats, merged\n",
        "\n",
        "def analyze_by_well(merged_data):\n",
        "    \"\"\"\n",
        "    Analiza estad√≠sticas por pozo individual\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    merged_data : pandas.DataFrame\n",
        "        Datos fusionados observados-simulados\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        Estad√≠sticas por pozo\n",
        "    \"\"\"\n",
        "    print(\"üîç Analizando estad√≠sticas por pozo...\")\n",
        "    \n",
        "    well_stats = []\n",
        "    \n",
        "    for well_id in merged_data['well_id'].unique():\n",
        "        well_data = merged_data[merged_data['well_id'] == well_id]\n",
        "        \n",
        "        if len(well_data) > 1:  # Necesitamos al menos 2 puntos para estad√≠sticas\n",
        "            stats = {\n",
        "                'well_id': well_id,\n",
        "                'n_obs': len(well_data),\n",
        "                'mae': well_data['abs_residual'].mean(),\n",
        "                'rmse': np.sqrt(well_data['squared_residual'].mean()),\n",
        "                'r2': 1 - (well_data['squared_residual'].sum() / \n",
        "                          ((well_data['head_obs'] - well_data['head_obs'].mean())**2).sum()),\n",
        "                'mean_residual': well_data['residual'].mean(),\n",
        "                'std_residual': well_data['residual'].std()\n",
        "            }\n",
        "            well_stats.append(stats)\n",
        "    \n",
        "    well_df = pd.DataFrame(well_stats)\n",
        "    print(f\"   Analizados {len(well_df)} pozos\")\n",
        "    \n",
        "    return well_df\n",
        "\n",
        "print(\"üìã Funciones de an√°lisis estad√≠stico definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Visualizaciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_time_series(merged_data, well_ids=None, save_path=None):\n",
        "    \"\"\"\n",
        "    Genera gr√°ficos de series temporales para pozos espec√≠ficos\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    merged_data : pandas.DataFrame\n",
        "        Datos fusionados observados-simulados\n",
        "    well_ids : list, optional\n",
        "        Lista de IDs de pozos a graficar\n",
        "    save_path : str, optional\n",
        "        Ruta para guardar el gr√°fico\n",
        "    \"\"\"\n",
        "    print(\"üìà Generando gr√°ficos de series temporales...\")\n",
        "    \n",
        "    if well_ids is None:\n",
        "        well_ids = merged_data['well_id'].unique()[:5]  # Primeros 5 pozos por defecto\n",
        "    \n",
        "    n_wells = len(well_ids)\n",
        "    fig, axes = plt.subplots(n_wells, 1, figsize=(12, 3*n_wells))\n",
        "    \n",
        "    if n_wells == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, well_id in enumerate(well_ids):\n",
        "        well_data = merged_data[merged_data['well_id'] == well_id].sort_values('time')\n",
        "        \n",
        "        axes[i].plot(well_data['time'], well_data['head_obs'], 'o-', \n",
        "                    label='Observado', color='blue', alpha=0.7)\n",
        "        axes[i].plot(well_data['time'], well_data['head_sim'], 's-', \n",
        "                    label='Simulado', color='red', alpha=0.7)\n",
        "        \n",
        "        axes[i].set_title(f'Pozo {well_id}')\n",
        "        axes[i].set_xlabel('Tiempo (d√≠as)')\n",
        "        axes[i].set_ylabel('Cabeza (m)')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   Gr√°fico guardado en: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter_comparison(merged_data, save_path=None):\n",
        "    \"\"\"\n",
        "    Genera gr√°fico de dispersi√≥n observado vs simulado\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    merged_data : pandas.DataFrame\n",
        "        Datos fusionados observados-simulados\n",
        "    save_path : str, optional\n",
        "        Ruta para guardar el gr√°fico\n",
        "    \"\"\"\n",
        "    print(\"üìä Generando gr√°fico de dispersi√≥n...\")\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "    \n",
        "    # Scatter plot\n",
        "    ax.scatter(merged_data['head_obs'], merged_data['head_sim'], \n",
        "              alpha=0.6, s=50, color='blue')\n",
        "    \n",
        "    # L√≠nea 1:1\n",
        "    min_val = min(merged_data['head_obs'].min(), merged_data['head_sim'].min())\n",
        "    max_val = max(merged_data['head_obs'].max(), merged_data['head_sim'].max())\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', \n",
        "           label='L√≠nea 1:1', linewidth=2)\n",
        "    \n",
        "    # L√≠nea de regresi√≥n\n",
        "    z = np.polyfit(merged_data['head_obs'], merged_data['head_sim'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax.plot(merged_data['head_obs'], p(merged_data['head_obs']), \n",
        "           'g-', label=f'Regresi√≥n (R¬≤={z[0]:.3f})', linewidth=2)\n",
        "    \n",
        "    ax.set_xlabel('Cabeza Observada (m)')\n",
        "    ax.set_ylabel('Cabeza Simulada (m)')\n",
        "    ax.set_title('Comparaci√≥n Observado vs Simulado')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   Gr√°fico guardado en: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def plot_residuals(merged_data, save_path=None):\n",
        "    \"\"\"\n",
        "    Genera gr√°ficos de an√°lisis de residuos\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    merged_data : pandas.DataFrame\n",
        "        Datos fusionados observados-simulados\n",
        "    save_path : str, optional\n",
        "        Ruta para guardar el gr√°fico\n",
        "    \"\"\"\n",
        "    print(\"üìâ Generando an√°lisis de residuos...\")\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # 1. Residuos vs tiempo\n",
        "    axes[0,0].scatter(merged_data['time'], merged_data['residual'], alpha=0.6)\n",
        "    axes[0,0].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[0,0].set_xlabel('Tiempo (d√≠as)')\n",
        "    axes[0,0].set_ylabel('Residuo (m)')\n",
        "    axes[0,0].set_title('Residuos vs Tiempo')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Residuos vs valores observados\n",
        "    axes[0,1].scatter(merged_data['head_obs'], merged_data['residual'], alpha=0.6)\n",
        "    axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[0,1].set_xlabel('Cabeza Observada (m)')\n",
        "    axes[0,1].set_ylabel('Residuo (m)')\n",
        "    axes[0,1].set_title('Residuos vs Valores Observados')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Histograma de residuos\n",
        "    axes[1,0].hist(merged_data['residual'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[1,0].axvline(x=0, color='r', linestyle='--')\n",
        "    axes[1,0].set_xlabel('Residuo (m)')\n",
        "    axes[1,0].set_ylabel('Frecuencia')\n",
        "    axes[1,0].set_title('Distribuci√≥n de Residuos')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Q-Q plot\n",
        "    from scipy import stats\n",
        "    stats.probplot(merged_data['residual'], dist=\"norm\", plot=axes[1,1])\n",
        "    axes[1,1].set_title('Q-Q Plot de Residuos')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   Gr√°fico guardado en: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "print(\"üìã Funciones de visualizaci√≥n definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ejemplo de Uso con Datos Sint√©ticos\n",
        "\n",
        "### 3.1 Generaci√≥n de datos de ejemplo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar datos sint√©ticos para demostraci√≥n\n",
        "print(\"üîß Generando datos sint√©ticos para demostraci√≥n...\")\n",
        "\n",
        "# Par√°metros del ejemplo\n",
        "n_wells = 5\n",
        "n_times = 100\n",
        "time_start = 0\n",
        "time_end = 365\n",
        "\n",
        "# Generar datos observados (con ruido)\n",
        "np.random.seed(42)  # Para reproducibilidad\n",
        "\n",
        "observed_data = []\n",
        "for well_id in range(1, n_wells + 1):\n",
        "    # Tendencia base diferente para cada pozo\n",
        "    base_head = 100 + well_id * 5  # Diferentes niveles base\n",
        "    seasonal = 2 * np.sin(2 * np.pi * np.linspace(0, 1, n_times))  # Variaci√≥n estacional\n",
        "    trend = 0.1 * np.linspace(0, 1, n_times)  # Tendencia temporal\n",
        "    noise = np.random.normal(0, 0.5, n_times)  # Ruido aleatorio\n",
        "    \n",
        "    times = np.linspace(time_start, time_end, n_times)\n",
        "    heads = base_head + seasonal + trend + noise\n",
        "    \n",
        "    for t, h in zip(times, heads):\n",
        "        observed_data.append({\n",
        "            'well_id': f'PZ{well_id:02d}',\n",
        "            'time': t,\n",
        "            'head': h\n",
        "        })\n",
        "\n",
        "observed_df = pd.DataFrame(observed_data)\n",
        "\n",
        "# Generar datos simulados (con algunos errores sistem√°ticos)\n",
        "simulated_data = []\n",
        "for well_id in range(1, n_wells + 1):\n",
        "    well_obs = observed_df[observed_df['well_id'] == f'PZ{well_id:02d}']\n",
        "    \n",
        "    for _, row in well_obs.iterrows():\n",
        "        # Simular error sistem√°tico y aleatorio\n",
        "        systematic_error = 0.5 if well_id % 2 == 0 else -0.3  # Error sistem√°tico por pozo\n",
        "        random_error = np.random.normal(0, 0.3)\n",
        "        simulated_head = row['head'] + systematic_error + random_error\n",
        "        \n",
        "        simulated_data.append({\n",
        "            'well_id': row['well_id'],\n",
        "            'time': row['time'],\n",
        "            'head': simulated_head\n",
        "        })\n",
        "\n",
        "simulated_df = pd.DataFrame(simulated_data)\n",
        "\n",
        "print(f\"‚úÖ Datos sint√©ticos generados:\")\n",
        "print(f\"   Pozos: {n_wells}\")\n",
        "print(f\"   Observaciones por pozo: {n_times}\")\n",
        "print(f\"   Total observaciones: {len(observed_df)}\")\n",
        "print(f\"   Rango de cabezas observadas: {observed_df['head'].min():.2f} - {observed_df['head'].max():.2f} m\")\n",
        "print(f\"   Rango de cabezas simuladas: {simulated_df['head'].min():.2f} - {simulated_df['head'].max():.2f} m\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Aplicaci√≥n del flujo de trabajo completo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PASO 1: Limpiar datos observados\n",
        "print(\"=\" * 60)\n",
        "print(\"PASO 1: PRE-PROCESAMIENTO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "observed_clean = clean_observation_data(observed_df)\n",
        "simulated_clean = clean_observation_data(simulated_df)\n",
        "\n",
        "# PASO 2: Calcular estad√≠sticas\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PASO 2: AN√ÅLISIS ESTAD√çSTICO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "stats, merged_data = calculate_statistics(observed_clean, simulated_clean)\n",
        "well_stats = analyze_by_well(merged_data)\n",
        "\n",
        "# Mostrar estad√≠sticas por pozo\n",
        "print(\"\\nüìä Estad√≠sticas por pozo:\")\n",
        "print(well_stats.round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PASO 3: Generar visualizaciones\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PASO 3: VISUALIZACIONES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Gr√°fico de series temporales\n",
        "plot_time_series(merged_data, well_ids=['PZ01', 'PZ02', 'PZ03'], \n",
        "                save_path=FIGURES_DIR / \"time_series.png\")\n",
        "\n",
        "# Gr√°fico de dispersi√≥n\n",
        "plot_scatter_comparison(merged_data, \n",
        "                       save_path=FIGURES_DIR / \"scatter_comparison.png\")\n",
        "\n",
        "# An√°lisis de residuos\n",
        "plot_residuals(merged_data, \n",
        "              save_path=FIGURES_DIR / \"residual_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PASO 4: Guardar resultados\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PASO 4: GUARDAR RESULTADOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Guardar datos procesados\n",
        "observed_clean.to_csv(DATA_PROCESSED / \"observed_clean.csv\", index=False)\n",
        "simulated_clean.to_csv(DATA_PROCESSED / \"simulated_clean.csv\", index=False)\n",
        "merged_data.to_csv(DATA_OUTPUT / \"merged_data.csv\", index=False)\n",
        "well_stats.to_csv(DATA_OUTPUT / \"well_statistics.csv\", index=False)\n",
        "\n",
        "# Guardar estad√≠sticas globales\n",
        "stats_df = pd.DataFrame([stats])\n",
        "stats_df.to_csv(DATA_OUTPUT / \"global_statistics.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Archivos guardados:\")\n",
        "print(f\"   Datos observados limpios: {DATA_PROCESSED / 'observed_clean.csv'}\")\n",
        "print(f\"   Datos simulados limpios: {DATA_PROCESSED / 'simulated_clean.csv'}\")\n",
        "print(f\"   Datos fusionados: {DATA_OUTPUT / 'merged_data.csv'}\")\n",
        "print(f\"   Estad√≠sticas por pozo: {DATA_OUTPUT / 'well_statistics.csv'}\")\n",
        "print(f\"   Estad√≠sticas globales: {DATA_OUTPUT / 'global_statistics.csv'}\")\n",
        "print(f\"   Gr√°ficos: {FIGURES_DIR}\")\n",
        "\n",
        "# Resumen final\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RESUMEN FINAL\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üìä Total de observaciones analizadas: {stats['n_observations']}\")\n",
        "print(f\"üìà Error absoluto medio (MAE): {stats['mae']:.3f} m\")\n",
        "print(f\"üìâ Error cuadr√°tico medio (RMSE): {stats['rmse']:.3f} m\")\n",
        "print(f\"üéØ Coeficiente de determinaci√≥n (R¬≤): {stats['r2']:.3f}\")\n",
        "print(f\"‚ö° Eficiencia Nash-Sutcliffe (NSE): {stats['nse']:.3f}\")\n",
        "\n",
        "# Interpretaci√≥n de resultados\n",
        "print(\"\\nüìã Interpretaci√≥n de resultados:\")\n",
        "if stats['nse'] > 0.7:\n",
        "    print(\"   ‚úÖ Excelente ajuste del modelo (NSE > 0.7)\")\n",
        "elif stats['nse'] > 0.5:\n",
        "    print(\"   ‚ö†Ô∏è  Ajuste aceptable del modelo (NSE > 0.5)\")\n",
        "else:\n",
        "    print(\"   ‚ùå Ajuste deficiente del modelo (NSE < 0.5)\")\n",
        "\n",
        "if stats['r2'] > 0.8:\n",
        "    print(\"   ‚úÖ Alta correlaci√≥n entre observado y simulado (R¬≤ > 0.8)\")\n",
        "elif stats['r2'] > 0.6:\n",
        "    print(\"   ‚ö†Ô∏è  Correlaci√≥n moderada (R¬≤ > 0.6)\")\n",
        "else:\n",
        "    print(\"   ‚ùå Baja correlaci√≥n (R¬≤ < 0.6)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Funciones Auxiliares para Archivos MODFLOW\n",
        "\n",
        "### 4.1 Procesamiento de archivos .hds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_head_at_wells(head_array, well_locations, nrow, ncol):\n",
        "    \"\"\"\n",
        "    Extrae valores de cabeza en ubicaciones espec√≠ficas de pozos\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    head_array : numpy.ndarray\n",
        "        Array 3D con cabezas\n",
        "    well_locations : list\n",
        "        Lista de tuplas (row, col) para cada pozo\n",
        "    nrow, ncol : int\n",
        "        Dimensiones de la grilla\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame con cabezas por pozo y estrato\n",
        "    \"\"\"\n",
        "    print(\"üîç Extrayendo cabezas en ubicaciones de pozos...\")\n",
        "    \n",
        "    well_data = []\n",
        "    \n",
        "    for i, (row, col) in enumerate(well_locations):\n",
        "        if 0 <= row < nrow and 0 <= col < ncol:\n",
        "            for layer in range(head_array.shape[0]):\n",
        "                head_value = head_array[layer, row, col]\n",
        "                well_data.append({\n",
        "                    'well_id': f'PZ{i+1:02d}',\n",
        "                    'layer': layer + 1,\n",
        "                    'row': row + 1,  # Convertir a √≠ndices 1-based\n",
        "                    'col': col + 1,\n",
        "                    'head': head_value\n",
        "                })\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Ubicaci√≥n de pozo {i+1} fuera de rango: ({row}, {col})\")\n",
        "    \n",
        "    df = pd.DataFrame(well_data)\n",
        "    print(f\"   ‚úÖ Extra√≠das {len(df)} cabezas de {len(well_locations)} pozos\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def create_head_contour_plot(head_array, layer=0, save_path=None):\n",
        "    \"\"\"\n",
        "    Crea mapa de contornos de cabezas\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    head_array : numpy.ndarray\n",
        "        Array 3D con cabezas\n",
        "    layer : int\n",
        "        Estrato a visualizar\n",
        "    save_path : str, optional\n",
        "        Ruta para guardar el gr√°fico\n",
        "    \"\"\"\n",
        "    print(f\"üó∫Ô∏è  Generando mapa de contornos para estrato {layer+1}...\")\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "    \n",
        "    # Crear contornos\n",
        "    contour = ax.contour(head_array[layer], levels=20, colors='black', alpha=0.6)\n",
        "    contour_filled = ax.contourf(head_array[layer], levels=20, cmap='viridis', alpha=0.8)\n",
        "    \n",
        "    # A√±adir barra de color\n",
        "    cbar = plt.colorbar(contour_filled, ax=ax)\n",
        "    cbar.set_label('Cabeza (m)', rotation=270, labelpad=20)\n",
        "    \n",
        "    ax.set_title(f'Mapa de Contornos de Cabezas - Estrato {layer+1}')\n",
        "    ax.set_xlabel('Columna')\n",
        "    ax.set_ylabel('Fila')\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   Mapa guardado en: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "print(\"üìã Funciones auxiliares para archivos MODFLOW definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Flujo de Trabajo Completo\n",
        "\n",
        "### 5.1 Funci√≥n principal de procesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_modflow_data(observed_file=None, simulated_file=None, head_file=None, \n",
        "                        well_locations=None, nrow=None, ncol=None, \n",
        "                        output_dir=None, project_name=\"modflow_analysis\"):\n",
        "    \"\"\"\n",
        "    Funci√≥n principal para procesar datos de MODFLOW\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    observed_file : str, optional\n",
        "        Ruta al archivo de observaciones\n",
        "    simulated_file : str, optional\n",
        "        Ruta al archivo de datos simulados\n",
        "    head_file : str, optional\n",
        "        Ruta al archivo .hds\n",
        "    well_locations : list, optional\n",
        "        Ubicaciones de pozos [(row, col), ...]\n",
        "    nrow, ncol : int, optional\n",
        "        Dimensiones de la grilla\n",
        "    output_dir : str, optional\n",
        "        Directorio de salida\n",
        "    project_name : str\n",
        "        Nombre del proyecto\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Diccionario con resultados del an√°lisis\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Iniciando procesamiento completo de datos MODFLOW\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Configurar directorios\n",
        "    if output_dir is None:\n",
        "        output_dir = Path.cwd() / \"output\" / project_name\n",
        "    \n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    results = {\n",
        "        'project_name': project_name,\n",
        "        'output_dir': str(output_dir),\n",
        "        'processed_files': [],\n",
        "        'statistics': {},\n",
        "        'figures': []\n",
        "    }\n",
        "    \n",
        "    # PASO 1: Procesar datos observados\n",
        "    if observed_file and Path(observed_file).exists():\n",
        "        print(\"üìä Procesando datos observados...\")\n",
        "        observed_df = read_observation_file(observed_file)\n",
        "        if observed_df is not None:\n",
        "            observed_clean = clean_observation_data(observed_df)\n",
        "            observed_clean.to_csv(output_dir / \"observed_clean.csv\", index=False)\n",
        "            results['processed_files'].append(\"observed_clean.csv\")\n",
        "            print(\"   ‚úÖ Datos observados procesados\")\n",
        "    \n",
        "    # PASO 2: Procesar datos simulados\n",
        "    if simulated_file and Path(simulated_file).exists():\n",
        "        print(\"üîß Procesando datos simulados...\")\n",
        "        simulated_df = read_observation_file(simulated_file)\n",
        "        if simulated_df is not None:\n",
        "            simulated_clean = clean_observation_data(simulated_df)\n",
        "            simulated_clean.to_csv(output_dir / \"simulated_clean.csv\", index=False)\n",
        "            results['processed_files'].append(\"simulated_clean.csv\")\n",
        "            print(\"   ‚úÖ Datos simulados procesados\")\n",
        "    \n",
        "    # PASO 3: Procesar archivos de cabezas\n",
        "    if head_file and Path(head_file).exists() and nrow and ncol:\n",
        "        print(\"üóÇÔ∏è  Procesando archivo de cabezas...\")\n",
        "        head_array = read_modflow_head_file(head_file, nrow, ncol)\n",
        "        if head_array is not None:\n",
        "            # Validar datos\n",
        "            head_stats = validate_head_data(head_array)\n",
        "            results['head_statistics'] = head_stats\n",
        "            \n",
        "            # Extraer cabezas en pozos si se proporcionan ubicaciones\n",
        "            if well_locations:\n",
        "                well_heads = extract_head_at_wells(head_array, well_locations, nrow, ncol)\n",
        "                well_heads.to_csv(output_dir / \"well_heads.csv\", index=False)\n",
        "                results['processed_files'].append(\"well_heads.csv\")\n",
        "            \n",
        "            # Crear mapa de contornos\n",
        "            create_head_contour_plot(head_array, layer=0, \n",
        "                                   save_path=output_dir / \"head_contour.png\")\n",
        "            results['figures'].append(\"head_contour.png\")\n",
        "            print(\"   ‚úÖ Archivo de cabezas procesado\")\n",
        "    \n",
        "    # PASO 4: An√°lisis estad√≠stico (si tenemos datos observados y simulados)\n",
        "    if 'observed_clean' in locals() and 'simulated_clean' in locals():\n",
        "        print(\"üìà Realizando an√°lisis estad√≠stico...\")\n",
        "        stats, merged_data = calculate_statistics(observed_clean, simulated_clean)\n",
        "        well_stats = analyze_by_well(merged_data)\n",
        "        \n",
        "        # Guardar resultados\n",
        "        merged_data.to_csv(output_dir / \"merged_data.csv\", index=False)\n",
        "        well_stats.to_csv(output_dir / \"well_statistics.csv\", index=False)\n",
        "        stats_df = pd.DataFrame([stats])\n",
        "        stats_df.to_csv(output_dir / \"global_statistics.csv\", index=False)\n",
        "        \n",
        "        results['statistics'] = stats\n",
        "        results['processed_files'].extend([\"merged_data.csv\", \"well_statistics.csv\", \"global_statistics.csv\"])\n",
        "        \n",
        "        # Generar visualizaciones\n",
        "        print(\"üìä Generando visualizaciones...\")\n",
        "        plot_time_series(merged_data, save_path=output_dir / \"time_series.png\")\n",
        "        plot_scatter_comparison(merged_data, save_path=output_dir / \"scatter_comparison.png\")\n",
        "        plot_residuals(merged_data, save_path=output_dir / \"residual_analysis.png\")\n",
        "        \n",
        "        results['figures'].extend([\"time_series.png\", \"scatter_comparison.png\", \"residual_analysis.png\"])\n",
        "        print(\"   ‚úÖ An√°lisis estad√≠stico completado\")\n",
        "    \n",
        "    # PASO 5: Generar reporte\n",
        "    print(\"üìù Generando reporte...\")\n",
        "    generate_report(results, output_dir / \"analysis_report.txt\")\n",
        "    results['processed_files'].append(\"analysis_report.txt\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ PROCESAMIENTO COMPLETADO\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìÅ Directorio de salida: {output_dir}\")\n",
        "    print(f\"üìä Archivos procesados: {len(results['processed_files'])}\")\n",
        "    print(f\"üìà Gr√°ficos generados: {len(results['figures'])}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "def generate_report(results, report_path):\n",
        "    \"\"\"\n",
        "    Genera un reporte de texto con los resultados del an√°lisis\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    results : dict\n",
        "        Resultados del an√°lisis\n",
        "    report_path : str\n",
        "        Ruta para guardar el reporte\n",
        "    \"\"\"\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"REPORTE DE AN√ÅLISIS MODFLOW\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"Proyecto: {results['project_name']}\\n\")\n",
        "        f.write(f\"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "        \n",
        "        f.write(\"ARCHIVOS PROCESADOS:\\n\")\n",
        "        f.write(\"-\" * 30 + \"\\n\")\n",
        "        for file in results['processed_files']:\n",
        "            f.write(f\"‚Ä¢ {file}\\n\")\n",
        "        \n",
        "        f.write(\"\\nGR√ÅFICOS GENERADOS:\\n\")\n",
        "        f.write(\"-\" * 30 + \"\\n\")\n",
        "        for fig in results['figures']:\n",
        "            f.write(f\"‚Ä¢ {fig}\\n\")\n",
        "        \n",
        "        if 'statistics' in results and results['statistics']:\n",
        "            f.write(\"\\nESTAD√çSTICAS GLOBALES:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            stats = results['statistics']\n",
        "            f.write(f\"Observaciones: {stats.get('n_observations', 'N/A')}\\n\")\n",
        "            f.write(f\"MAE: {stats.get('mae', 'N/A'):.3f} m\\n\")\n",
        "            f.write(f\"RMSE: {stats.get('rmse', 'N/A'):.3f} m\\n\")\n",
        "            f.write(f\"R¬≤: {stats.get('r2', 'N/A'):.3f}\\n\")\n",
        "            f.write(f\"NSE: {stats.get('nse', 'N/A'):.3f}\\n\")\n",
        "    \n",
        "    print(f\"   üìÑ Reporte guardado en: {report_path}\")\n",
        "\n",
        "print(\"üìã Funci√≥n principal de procesamiento definida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Instrucciones de Uso\n",
        "\n",
        "### 6.1 Para usar con datos reales:\n",
        "\n",
        "```python\n",
        "# Ejemplo de uso con archivos reales\n",
        "results = process_modflow_data(\n",
        "    observed_file=\"data/raw/observations.obs\",\n",
        "    simulated_file=\"data/raw/simulated.obs\", \n",
        "    head_file=\"data/raw/model.hds\",\n",
        "    well_locations=[(10, 15), (20, 25), (30, 35)],  # (fila, columna)\n",
        "    nrow=50, ncol=60,\n",
        "    output_dir=\"results/analysis_2024\",\n",
        "    project_name=\"mi_proyecto_modflow\"\n",
        ")\n",
        "```\n",
        "\n",
        "### 6.2 Estructura de archivos de entrada:\n",
        "\n",
        "**Archivo de observaciones (.obs):**\n",
        "```\n",
        "# Comentarios opcionales\n",
        "PZ01 0.0 105.2\n",
        "PZ01 30.0 104.8\n",
        "PZ02 0.0 98.5\n",
        "PZ02 30.0 98.1\n",
        "```\n",
        "\n",
        "**Archivo de cabezas (.hds):**\n",
        "- Archivo binario generado por MODFLOW\n",
        "- Requiere especificar dimensiones de la grilla (nrow, ncol)\n",
        "\n",
        "### 6.3 Interpretaci√≥n de resultados:\n",
        "\n",
        "- **MAE (Mean Absolute Error)**: Error absoluto medio en metros\n",
        "- **RMSE (Root Mean Square Error)**: Error cuadr√°tico medio en metros  \n",
        "- **R¬≤**: Coeficiente de determinaci√≥n (0-1, mayor es mejor)\n",
        "- **NSE (Nash-Sutcliffe Efficiency)**: Eficiencia del modelo (-‚àû a 1, mayor es mejor)\n",
        "\n",
        "**Criterios de evaluaci√≥n:**\n",
        "- NSE > 0.7: Excelente ajuste\n",
        "- NSE > 0.5: Ajuste aceptable  \n",
        "- NSE < 0.5: Ajuste deficiente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Archivos de Configuraci√≥n\n",
        "\n",
        "### 7.1 requirements.txt\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
